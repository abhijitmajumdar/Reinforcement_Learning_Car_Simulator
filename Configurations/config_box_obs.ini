[Reinforcement Parameters]
epsilon = 0.0
max_epsilon = 0.9
epsilon_step = 0.005
gamma = 0.99
lr_alpha = 0.001
leak_alpha = 0.3
terminal_state_rewards = {'collided':-1,'destination':1}
normal_reward = -0.0001
buffer_length = 300000
replay_start_at = 10000
batchsize = 256
minibatchsize = 256
save_interval = 20
random_seed = None
target_network_update_frequency = 50

[Actions]
slow forward = 0.2,0.0
slow left  =  0.2,0.6
slow right =  0.2,-0.6
fast forward = 1.5,0.0
fast left  =  1.5,0.1
fast right =  1.5,-0.1

[Network]
activation = None
layers = 20,12

[Process triggers]
random_agent_position = True
random_destination_position = True

[Log]
logdir = './weights/'

[Arena]
dest_radius = 0.5
buffer_space = 0.7
dt = 0.1
max_steps = 1000
graphs = ['Average loss','Total reward','Running reward']
display_resolution = [1280,720]
display_dk = 0.005
trace_history_limit = 100
track_width = 0.8
path_creator=False
no_obstacles=False
BOX = [(0,0),(10,0),(10,10),(0,10)]

[Obstacle]
a = [(4.5,4.5),(5.5,4.5),(5.5,5.5),(4.5,5.5)]

[Cars]
  [[1]]
    state = 1,0.3,0
    L = 0.3
    W = 0.1
    v_limit = 2
    gamma_limit = 0.61
    destination = (11.5,0)
    connection = 1
    [[[sensors]]]
      [[[[S1]]]]
        range = 2.0
        angle = 0.53
      [[[[S2]]]]
        range = 2.0
        angle = 0.0
      [[[[S3]]]]
        range = 2.0
        angle = -0.53
