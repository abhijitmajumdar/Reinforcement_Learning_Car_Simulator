[Reinforcement Parameters]
epsilon = 0.0
max_epsilon = 0.9
epsilon_step = 0.005
gamma = 0.99
lr_alpha = 0.001
leak_alpha = 0.3
terminal_state_rewards = {'collided':-1,'destination':1}
normal_reward = -0.0001
buffer_length = 300000
replay_start_at = 10000
batchsize = 256
minibatchsize = 256
save_interval = 20
random_seed = None
target_network_update_frequency = 50

[Actions]
slow forward = 0.2,0.0
slow left  =  0.2,0.6
slow right =  0.2,-0.6
fast forward = 1.5,0.0
fast left  =  1.5,0.1
fast right =  1.5,-0.1

[Network]
activation = None
layers = 20,12

[Process triggers]
random_agent_position = False
random_destination_position = False

[Log]
logdir = './weights/'

[Arena]
dest_radius = 0.8
buffer_space = 0.7
dt = 0.1
max_steps = 1000
graphs = ['Average loss','Total reward','Running reward']
display_resolution = [1280,720]
display_dk = 0.005
trace_history_limit = 100
track_width = 0.8
path_creator=True
no_obstacles=True
TRACK = [(0,0.5),(5,0),(7,2),(5,4),(2,4),(0,6),(2,8),(12,8),(14,6),(12,4),(10,4),(9,2),(10,0),(12,0)]

[Obstacle]
a = [(4.5,4.5),(5.5,4.5),(5.5,5.5),(4.5,5.5)]
#b = [(8,8),(9,8),(9,9),(8,9)]
#c = [(9,1.5),(10,1.5),(10,2.5),(9,2.5)]
#d = [(8.0, 24.0), (8.38, 21.73), (9.48, 19.7), (11.17, 18.14), (13.28, 17.21), (15.58, 17.02), (17.81, 17.59), (19.74, 18.85), (21.16, 20.67), (21.9, 22.85), (21.9, 25.15), (21.16, 27.33), (19.74, 29.15), (17.81, 30.41), (15.58, 30.98), (13.28, 30.79), (11.17, 29.86), (9.48, 28.3), (8.38, 26.27), (8.0, 24.0)]


[Cars]
  [[1]]
    state = 1,0.3,0
    L = 0.3
    W = 0.1
    v_limit = 2
    gamma_limit = 0.61
    destination = (11.5,0)
    connection = 1
    [[[sensors]]]
      [[[[S1]]]]
        range = 2.0
        angle = 0.53
      [[[[S2]]]]
        range = 2.0
        angle = 0.0
      [[[[S3]]]]
        range = 2.0
        angle = -0.53
